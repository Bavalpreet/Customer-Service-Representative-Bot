{
  "mood_unhappy": {
    "precision": 0.5625,
    "recall": 0.6428571428571429,
    "f1-score": 0.6000000000000001,
    "support": 14,
    "confused_with": {
      "mood_great": 2,
      "nlu-faq-clay-country-b1": 1
    }
  },
  "nlu-faq-weho-parking-b1": {
    "precision": 0.9902912621359223,
    "recall": 0.9886914378029079,
    "f1-score": 0.9894907033144704,
    "support": 619,
    "confused_with": {
      "nlu-faq-clay-country-b1": 7
    }
  },
  "mood_great": {
    "precision": 0.6923076923076923,
    "recall": 0.6428571428571429,
    "f1-score": 0.6666666666666666,
    "support": 14,
    "confused_with": {
      "mood_unhappy": 2,
      "nlu-faq-weho-parking-b1": 1
    }
  },
  "goodbye": {
    "precision": 1.0,
    "recall": 0.7,
    "f1-score": 0.8235294117647058,
    "support": 10,
    "confused_with": {
      "nlu-faq-utah-adult": 1,
      "mood_unhappy": 1
    }
  },
  "nlu-faq-clay-country-b1": {
    "precision": 0.9880873593646592,
    "recall": 0.9959973315543695,
    "f1-score": 0.9920265780730897,
    "support": 1499,
    "confused_with": {
      "nlu-faq-weho-parking-b1": 4,
      "nlu-faq-utah-adult": 2
    }
  },
  "affirm": {
    "precision": 0.3333333333333333,
    "recall": 0.3333333333333333,
    "f1-score": 0.3333333333333333,
    "support": 6,
    "confused_with": {
      "nlu-faq-clay-country-b1": 2,
      "nlu-faq-utah-adult": 1
    }
  },
  "deny": {
    "precision": 0.3333333333333333,
    "recall": 0.2857142857142857,
    "f1-score": 0.30769230769230765,
    "support": 7,
    "confused_with": {
      "mood_unhappy": 4,
      "affirm": 1
    }
  },
  "bot_challenge": {
    "precision": 1.0,
    "recall": 0.75,
    "f1-score": 0.8571428571428571,
    "support": 4,
    "confused_with": {
      "nlu-faq-utah-adult": 1
    }
  },
  "nlu-faq-utah-adult": {
    "precision": 0.9724770642201835,
    "recall": 0.9592760180995475,
    "f1-score": 0.9658314350797267,
    "support": 221,
    "confused_with": {
      "nlu-faq-clay-country-b1": 8,
      "nlu-faq-weho-parking-b1": 1
    }
  },
  "greet": {
    "precision": 1.0,
    "recall": 0.6923076923076923,
    "f1-score": 0.8181818181818181,
    "support": 13,
    "confused_with": {
      "deny": 2,
      "mood_great": 1
    }
  },
  "accuracy": 0.9796427087660988,
  "macro avg": {
    "precision": 0.7872330044695124,
    "recall": 0.6991034384526422,
    "f1-score": 0.7353895111248976,
    "support": 2407
  },
  "weighted avg": {
    "precision": 0.9796224824129878,
    "recall": 0.9796427087660988,
    "f1-score": 0.9793015129645198,
    "support": 2407
  }
}